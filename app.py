import inspect
import json

from contextlib import asynccontextmanager
from collections.abc import AsyncGenerator
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langgraph.types import Interrupt
from langgraph.pregel import Pregel
from typing import Any, Union
from uuid import UUID, uuid4

from src.agents.agents import (
    DEFAULT_AGENT, 
    PMHC_AFFINITY_PREDICTION, 
    PATIENT_CASE_MRNA_AGENT,
    NEO_ANTIGEN,
    PREDICT_NEO_ANTIGEN,
    get_all_agent_info,
    get_all_agents,
    get_agent
)
from src.agents.files.file_description import fileDescriptionAgent
from src.agents.files.patient_info_formatter import PatientInfoDescriptionAgent
from src.agents.tools.parameters import ToolParameters

from src.memory import initialize_store, initialize_database
from src.schema.schema import (
    UserInput, 
    MinioRequest, 
    MinioResponse, 
    PatientInfoRequest, 
    PatientInfoResponse,
    PredictUserInput
)
from src.utils.message_handling import (
    convert_message_content_to_string,
    langchain_to_chat_message,
    remove_tool_calls,
    _sse_response_example
)
from src.utils.log import logger

logger.info(f"========================start molly_langgraph backend==============================")
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Configurable lifespan that initializes the appropriate database checkpointer and store
    based on settings.
    """
    try:
        # Initialize both checkpointer (for short-term memory) and store (for long-term memory)
        async with initialize_database() as saver, initialize_store() as store:
            # Set up both components
            if hasattr(saver, "setup"):  # ignore: union-attr
                await saver.setup()
            # Only setup store for Postgres as InMemoryStore doesn't need setup
            if hasattr(store, "setup"):  # ignore: union-attr
                await store.setup()

            # Configure agents with both memory components
            agents = get_all_agent_info()
            for a in agents:
                agent = get_agent(a.key)
                # Set checkpointer for thread-scoped memory (conversation history)
                agent.checkpointer = saver
                # Set store for long-term memory (cross-conversation knowledge)
                agent.store = store
            yield
    except Exception as e:
        logger.error(f"Error during database/store initialization: {e}")
        raise

app = FastAPI(lifespan=lifespan)

origins = [
    "*",  # å…è®¸çš„æ¥æºï¼Œå¯ä»¥æ·»åŠ å¤šä¸ª
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # å…è®¸è®¿é—®çš„æºåˆ—è¡¨
    allow_credentials=True,  # æ”¯æŒcookieè·¨åŸŸ
    allow_methods=["*"],  # å…è®¸çš„è¯·æ±‚æ–¹æ³•
    allow_headers=["*"],  # å…è®¸çš„è¯·æ±‚å¤´
)

load_dotenv()

def _parse_input(user_input: Union[UserInput, PredictUserInput]) -> tuple[dict[str, Any], UUID]:
    run_id = uuid4()
    thread_id = user_input.conversation_id
    
    # åŸºç¡€é…ç½®
    configurable = {
        "thread_id": thread_id, 
    }
    
    # æ ¹æ®è¾“å…¥ç±»å‹æ·»åŠ ç‰¹å®šé…ç½®
    if isinstance(user_input, UserInput):
        configurable["file_list"] = user_input.file_list
    elif isinstance(user_input, PredictUserInput):
        configurable.update(
            {
                "file_path": user_input.file_path,
                "mhc_allele": user_input.mhc_allele,
                "cdr3": user_input.cdr3,
                "patient_id": user_input.patient_id,
                "predict_id": user_input.predict_id,
                "conversation_id" : user_input.conversation_id,
            }
        )
        # if user_input.parameters:
        tool_parameters = ToolParameters(**user_input.parameters)
        configurable.update(
            {
                "tool_parameters": tool_parameters,
            }
        )

    # if user_input.agent_config:
    #     if overlap := configurable.keys() & user_input.agent_config.keys():
    #         raise HTTPException(
    #             status_code=422,
    #             detail=f"agent_config contains reserved keys: {overlap}",
    #         )
    #     configurable.update(user_input.agent_config)    


    kwargs = {
        "input": {"messages": [HumanMessage(content=user_input.prompt)]},
        "config": RunnableConfig(
            configurable=configurable,
            run_id=run_id,
        ),
    }
    return kwargs, run_id

# async def message_generator(
#     user_input: UserInput, agent_id: str = DEFAULT_AGENT
# ) -> AsyncGenerator[str, None]:
#     """
#     Generate a stream of messages from the agent.

#     This is the workhorse method for the /stream endpoint.
#     """
#     agent: CompiledStateGraph = get_agent(agent_id)
#     kwargs, run_id = _parse_input(user_input)
#     #ä¸Šä¸€æ¬¡çš„yieldç±»å‹
#     previous_yield_type = "" 
#     #å®šä¹‰é˜Ÿåˆ—
#     tool_call_queue = deque()
#     # Process streamed events from the graph and yield messages over the SSE stream.
#     async for event in agent.astream_events(**kwargs, version="v2",stream_mode="custom"):
#         if not event:
#             continue
#         print("2222222222222222")
#         print(event)
#         new_messages = []
#         # Yield messages written to the graph state after node execution finishes.
#         if (
#             event["event"] == "on_chain_end"
#             # on_chain_end gets called a bunch of times in a graph execution
#             # This filters out everything except for "graph node finished"
#             and any(t.startswith("graph:step:") for t in event.get("tags", []))
#         ):
#             if isinstance(event["data"]["output"], Command):
#                 new_messages = event["data"]["output"].update.get("messages", [])
#             elif "messages" in event["data"]["output"]:
#                 new_messages = event["data"]["output"]["messages"]

#         # Also yield intermediate messages from agents.utils.CustomData.adispatch().
#         if event["event"] == "on_custom_event" and "custom_data_dispatch" in event.get("tags", []):
#             new_messages = [event["data"]]
       
#         for message in new_messages:
#             try:
#                 chat_message = langchain_to_chat_message(message)
#                 chat_message.run_id = str(run_id)
#             except Exception as e:
#                 logger.error(f"Error parsing message: {e}")
#                 yield f"data: {json.dumps({'type': 'error', 'content': 'Unexpected error'})}\n\n"
#                 continue

#             # è¿‡æ»¤æ‰ LangGraph é‡æ–°å‘é€çš„è¾“å…¥æ¶ˆæ¯
#             if chat_message.type == "human" and chat_message.content == user_input.prompt:
#                 continue

#             # # å¤„ç† tool ç±»å‹çš„æ¶ˆæ¯
#             # if chat_message.type == "tool":
#             #     try:
#             #         # è§£æ content å­—æ®µä¸­çš„å­—ç¬¦ä¸²ä¸ºå­—å…¸
#             #         content_dict = ast.literal_eval(chat_message.content)
#             #         # ç›´æ¥å°†å­—å…¸èµ‹å€¼ç»™ contentï¼Œè€Œä¸æ˜¯è½¬æ¢ä¸ºå­—ç¬¦ä¸²
#             #         chat_message.content = content_dict
#             #     except (SyntaxError, ValueError, json.JSONDecodeError) as e:
#             #         logger.error(f"è§£æ tool content å¤±è´¥: {e}")
#             #         chat_message.content = {"type": "error", "content": "Invalid tool content format"}
#             # if chat_message.type == "tool":
#             #     try:
#             #         content_dict=json.loads(chat_message.content)
#             #         chat_message.content = content_dict
#             #     except (SyntaxError, ValueError, json.JSONDecodeError) as e:
#             #         logger.error(f"è§£æ tool content å¤±è´¥: {e}")
#             #         chat_message.content = {"type": "error", "content": "Invalid tool content format"}
#             # print(f"data: {json.dumps({'type': 'message', 'content': chat_message.model_dump()}, ensure_ascii=False)}\n\n")
   
#             if chat_message.type == "ai" and getattr(chat_message, "tool_calls", None) and previous_yield_type=="token":
#                 # å…ˆ yield ä¸€ä¸ªæ¢è¡Œç¬¦æ¶ˆæ¯
#                 newline = "\n"
#                 previous_yield_type="token" 
#                 yield f"data: {json.dumps({'type': 'token', 'content': newline}, ensure_ascii=False)}\n\n"
#             previous_yield_type="message"  

#             #å¤„ç†å•è½®ï¼Œå¤šä¸ªå·¥å…·è°ƒç”¨çš„å‰ç«¯æ¸²æŸ“é—®é¢˜
#             if chat_message.type == "ai" and getattr(chat_message, "tool_calls", None):
#                 # å¤„ç†å·¥å…·è°ƒç”¨
#                 if len(chat_message.tool_calls) > 1:
#                     # å¦‚æœæœ‰å¤šä¸ªå·¥å…·è°ƒç”¨ï¼Œæ‹†åˆ†æˆå•ç‹¬çš„æ¶ˆæ¯å­˜å…¥é˜Ÿåˆ—
#                     print(chat_message)
#                     for tool_call in chat_message.tool_calls:
#                         # åˆ›å»ºå•ä¸ªå·¥å…·è°ƒç”¨çš„æ¶ˆæ¯å‰¯æœ¬
#                         single_tool_message = chat_message.copy()
#                         single_tool_message.tool_calls = [tool_call]
#                         tool_call_queue.append(single_tool_message)
                    
#                     # ä¸ç«‹å³yieldï¼Œç­‰å¾…toolæ¶ˆæ¯æ—¶å†å¤„ç†
#                     continue
#                 elif len(chat_message.tool_calls) == 1:
#                     # å¦‚æœåªæœ‰ä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼Œç›´æ¥å­˜å…¥é˜Ÿåˆ—
#                     tool_call_queue.append(chat_message)
#                     continue
            
#             elif chat_message.type == "tool":
#                 # å½“æ”¶åˆ°toolæ¶ˆæ¯æ—¶ï¼Œä»é˜Ÿåˆ—ä¸­å–å‡ºå¯¹åº”çš„è°ƒç”¨æ¶ˆæ¯
#                 if tool_call_queue:
#                     queued_message = tool_call_queue.popleft()
#                     # å…ˆyieldå·¥å…·è°ƒç”¨æ¶ˆæ¯
#                     yield f"data: {json.dumps({'type': 'message', 'content': queued_message.model_dump()}, ensure_ascii=False)}\n\n"
#                     # ç„¶åyieldå·¥å…·å“åº”æ¶ˆæ¯
#                     yield f"data: {json.dumps({'type': 'message', 'content': chat_message.model_dump()}, ensure_ascii=False)}\n\n"
#             else:
#             # å…¶ä»–æƒ…å†µç›´æ¥yield
#                 yield f"data: {json.dumps({'type': 'message', 'content': chat_message.model_dump()}, ensure_ascii=False)}\n\n"
      

#         # Yield tokens streamed from LLMs.
#         if (
#             event["event"] == "on_chat_model_stream"
#             and user_input.stream_tokens
#             and "llama_guard" not in event.get("tags", [])
#         ):
#             content = remove_tool_calls(event["data"]["chunk"].content)
#             if content:
#                 # Empty content in the context of OpenAI usually means
#                 # that the model is asking for a tool to be invoked.
#                 # So we only print non-empty content.
#                 previous_yield_type="token" 

#                 yield f"data: {json.dumps({'type': 'token', 'content': convert_message_content_to_string(content)})}\n\n"
#             continue
#         #æ•è·å·¥å…·å†…çš„å“åº”
#         if (
#             event.get("event") == "on_chain_stream"
#             and event.get("name") == "LangGraph"
#             and "chunk" in event.get("data", {})
#         ):
#             chunk_content = event["data"]["chunk"]
#             if chunk_content:  # ç¡®ä¿ chunk éç©º
#                 yield f"data: {json.dumps({'type': 'token', 'content': chunk_content}, ensure_ascii=False)}\n\n"   
#             continue         
#     previous_yield_type="DONE" 
#     yield "data: [DONE]\n\n"


def _create_ai_message(parts: dict) -> AIMessage:
    sig = inspect.signature(AIMessage)
    valid_keys = set(sig.parameters)
    filtered = {k: v for k, v in parts.items() if k in valid_keys}
    return AIMessage(**filtered)


async def message_generator(
    user_input: Union[UserInput, PredictUserInput], agent_id: str = DEFAULT_AGENT
) -> AsyncGenerator[str, None]:
    """
    Generate a stream of messages from the agent.

    This is the workhorse method for the /stream endpoint.
    Supports both UserInput and PredictUserInput types.

    Args:
        user_input: Either UserInput or PredictUserInput object containing the user's request
        agent_id: The ID of the agent to use (defaults to DEFAULT_AGENT)

    Returns:
        An async generator yielding SSE formatted messages
    """
    
    agent: Pregel = get_agent(agent_id)
    kwargs, run_id = _parse_input(user_input)

    #ä¸Šä¸€æ¬¡çš„yieldç±»å‹
    previous_yield_type = "" 

    try:
        NEO_counts=0
        NEO_RESPONSE_counts=0
        # Process streamed events from the graph and yield messages over the SSE stream.
        async for stream_event in agent.astream(
            **kwargs, stream_mode=["updates", "messages", "custom"]
        ):
            if not isinstance(stream_event, tuple):
                continue
            stream_mode, event = stream_event
            # print(stream_mode, event)
            new_messages = []
            if stream_mode == "updates":
                for node, updates in event.items():
                    # A simple approach to handle agent interrupts.
                    # In a more sophisticated implementation, we could add
                    # some structured ChatMessage type to return the interrupt value.
                    if node == "__interrupt__":
                        interrupt: Interrupt
                        for interrupt in updates:
                            new_messages.append(AIMessage(content=interrupt.value))
                        continue
                    updates = updates or {}
                    update_messages = updates.get("messages", [])
                    # special cases for using langgraph-supervisor library
                    if node == "supervisor":
                        # Get only the last AIMessage since supervisor includes all previous messages
                        ai_messages = [msg for msg in update_messages if isinstance(msg, AIMessage)]
                        if ai_messages:
                            update_messages = [ai_messages[-1]]
                    if node in ("research_expert", "math_expert"):
                        # By default the sub-agent output is returned as an AIMessage.
                        # Convert it to a ToolMessage so it displays in the UI as a tool response.
                        msg = ToolMessage(
                            content=update_messages[0].content,
                            name=node,
                            tool_call_id="",
                        )
                        update_messages = [msg]
                    new_messages.extend(update_messages)
            
            if stream_mode == "custom":
                if isinstance(event, dict):
                    #  yield f"data: {json.dumps({'type': 'writer_token', 'content': event})}\n\n"
                    #  print(event)
                     dumps_event = json.dumps(event,ensure_ascii=False,)
                     yield f"data: {json.dumps({'type': 'writer_token', 'content': dumps_event})}\n\n"
                    #  print(dumps_event)
                elif event == "#NEO#":
                    NEO_counts += 1 
                    yield f"data: {json.dumps({'type': 'table', 'content': event})}\n\n"
                elif NEO_counts % 2 != 0:    
                    yield f"data: {json.dumps({'type': 'table', 'content': event})}\n\n"
                elif event == "#NEO_RESPONSE#":
                    NEO_RESPONSE_counts += 1 
                    yield f"data: {json.dumps({'type': 'response_table', 'content': event})}\n\n"
                elif NEO_RESPONSE_counts % 2 != 0:    
                    yield f"data: {json.dumps({'type': 'token', 'content': event})}\n\n"
                else:
                    previous_yield_type="token"
                    yield f"data: {json.dumps({'type': 'token', 'content': event})}\n\n"
                # print(event)

                # yield f"data: {json.dumps({'type': 'token', 'content': event})}\n\n"
                # new_messages = [event]

            # LangGraph streaming may emit tuples: (field_name, field_value)
            # e.g. ('content', <str>), ('tool_calls', [ToolCall,...]), ('additional_kwargs', {...}), etc.
            # We accumulate only supported fields into `parts` and skip unsupported metadata.
            # More info at: https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/
            processed_messages = []
            current_message: dict[str, Any] = {}
            for message in new_messages:
                if isinstance(message, tuple):
                    key, value = message
                    # Store parts in temporary dict
                    current_message[key] = value
                else:
                    # Add complete message if we have one in progress
                    if current_message:
                        processed_messages.append(_create_ai_message(current_message))
                        current_message = {}
                    processed_messages.append(message)

            # Add any remaining message parts
            if current_message:
                processed_messages.append(_create_ai_message(current_message))
            for message in processed_messages:
                try:
                    chat_message = langchain_to_chat_message(message)
                    chat_message.run_id = str(run_id)
                except Exception as e:
                    logger.error(f"Error parsing message: {e}")
                    yield f"data: {json.dumps({'type': 'error', 'content': 'Unexpected error'})}\n\n"
                    continue
                # LangGraph re-sends the input message, which feels weird, so drop it
                if chat_message.type == "human" and chat_message.content == user_input.message:
                    continue
                if chat_message.type == "ai" and getattr(chat_message, "tool_calls", None) and previous_yield_type=="token":
                    # å…ˆ yield ä¸€ä¸ªæ¢è¡Œç¬¦æ¶ˆæ¯
                    newline = "\n"
                    previous_yield_type="token" 
                    yield f"data: {json.dumps({'type': 'token', 'content': newline}, ensure_ascii=False)}\n\n"
                previous_yield_type="message" 

                yield f"data: {json.dumps({'type': 'message', 'content': chat_message.model_dump()})}\n\n"

            if stream_mode == "messages":
                if not user_input.stream_tokens:
                    continue
                msg, metadata = event
                if "skip_stream" in metadata.get("tags", []):
                    continue
                # For some reason, astream("messages") causes non-LLM nodes to send extra messages.
                # Drop them.
                if not isinstance(msg, AIMessageChunk):
                    continue
                content = remove_tool_calls(msg.content)
                if content:
                    # Empty content in the context of OpenAI usually means
                    # that the model is asking for a tool to be invoked.
                    # So we only print non-empty content.
                    previous_yield_type="token" 
                    # print(event)
                    yield f"data: {json.dumps({'type': 'token', 'content': convert_message_content_to_string(content)})}\n\n"
    except Exception as e:
        logger.error(f"Error in message generator: {e}")
        print(f"\nâŒ ERROR in message_generator: {str(e)}")
        print("ğŸ”„ Full traceback:")
        import traceback
        traceback.print_exc()  # ç›´æ¥æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        
        # è¿”å›ç»™å®¢æˆ·ç«¯çš„é”™è¯¯ä¿¡æ¯ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ç®€åŒ–ï¼‰
        error_detail = {
            "type": "error",
            "content": "Internal server error",
            # å¼€å‘æ—¶å¯ä»¥åŒ…å«è¯¦ç»†ä¿¡æ¯ï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥ç§»é™¤
            "debug_info": str(e)  
        }
        yield f"data: {json.dumps(error_detail, ensure_ascii=False)}\n\n"

        # yield f"data: {json.dumps({'type': 'error', 'content': 'Internal server error'})}\n\n"
    finally:
        previous_yield_type="DONE"         
        yield "data: [DONE]\n\n"

#mRNA_researchçš„æ¥å£
@app.post("/mRNA_research_stream", response_class=StreamingResponse, responses=_sse_response_example())
async def chat(user_input: UserInput, agent_id: str = DEFAULT_AGENT) -> StreamingResponse:
    """
    Stream an agent's response to a user input, including intermediate messages and tokens.

    If agent_id is not provided, the default agent will be used.
    Use thread_id to persist and continue a multi-turn conversation. run_id kwarg
    is also attached to all messages for recording feedback.

    Set `stream_tokens=false` to return intermediate messages but not token-by-token.
    """


    return StreamingResponse(
        message_generator(user_input, agent_id),
        media_type="text/event-stream",
    )

#pMHC_affinity_predictionçš„æ¥å£
@app.post("/pmhc_affinity_prediction_stream", response_class=StreamingResponse, responses=_sse_response_example())
async def chat(user_input: UserInput, agent_id: str = PMHC_AFFINITY_PREDICTION) -> StreamingResponse:
    """
    Stream an agent's response to a user input, including intermediate messages and tokens.

    If agent_id is not provided, the default agent will be used.
    Use thread_id to persist and continue a multi-turn conversation. run_id kwarg
    is also attached to all messages for recording feedback.

    Set `stream_tokens=false` to return intermediate messages but not token-by-token.
    """


    return StreamingResponse(
        message_generator(user_input, agent_id),
        media_type="text/event-stream",
    )

#patient_case_mrna_streamçš„æ¥å£
@app.post("/patient_case_mrna_stream", response_class=StreamingResponse, responses=_sse_response_example())
async def chat(user_input: UserInput, agent_id: str = PATIENT_CASE_MRNA_AGENT) -> StreamingResponse:
    """
    Stream an agent's response to a user input, including intermediate messages and tokens.

    If agent_id is not provided, the default agent will be used.
    Use thread_id to persist and continue a multi-turn conversation. run_id kwarg
    is also attached to all messages for recording feedback.

    Set `stream_tokens=false` to return intermediate messages but not token-by-token.
    """


    return StreamingResponse(
        message_generator(user_input, agent_id),
        media_type="text/event-stream",
    )

#neo_antigen_streamçš„æ¥å£
@app.post("/neo_antigen_stream", response_class=StreamingResponse, responses=_sse_response_example())
async def chat(user_input: UserInput, agent_id: str = NEO_ANTIGEN) -> StreamingResponse:
    """
    Stream an agent's response to a user input, including intermediate messages and tokens.

    If agent_id is not provided, the default agent will be used.
    Use thread_id to persist and continue a multi-turn conversation. run_id kwarg
    is also attached to all messages for recording feedback.

    Set `stream_tokens=false` to return intermediate messages but not token-by-token.
    """
    logger.info(f"Received user_input: {user_input.dict()}")
    logger.info(f"Agent ID: {agent_id}")
    return StreamingResponse(
        message_generator(user_input, agent_id),
        media_type="text/event-stream",
    )

#predict_neo_antigen_streamçš„æ¥å£
@app.post("/predict_neo_antigen_stream", response_class=StreamingResponse, responses=_sse_response_example())
async def chat(user_input: PredictUserInput, agent_id: str = PREDICT_NEO_ANTIGEN) -> StreamingResponse:
    """
    Stream an agent's response to a user input, including intermediate messages and tokens.

    If agent_id is not provided, the default agent will be used.
    Use thread_id to persist and continue a multi-turn conversation. run_id kwarg
    is also attached to all messages for recording feedback.

    Set `stream_tokens=false` to return intermediate messages but not token-by-token.
    """
    logger.info(f"Received user_input: {user_input.dict()}")
    logger.info(f"Agent ID: {agent_id}")
    return StreamingResponse(
        message_generator(user_input, agent_id),
        media_type="text/event-stream",
    )

#å¯¹minioä¼ æ¥çš„æ–‡ä»¶è¿›è¡Œæè¿°
@app.post("/description", response_model=MinioResponse)
async def describe_text(request: MinioRequest):
    try:
        # è°ƒç”¨ LangChain å¤„ç†é“¾
        result = fileDescriptionAgent.invoke({"file_name": request.file_name,"file_content":request.file_content})
        # è¿”å›åˆ†æç»“æœ
        return MinioResponse(file_description=result.content)
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=str(e))

#åˆ é™¤graphä¸­å›¾çš„çŠ¶æ€
@app.delete("/delete_thread/{thread_id}")
async def reset_thread(thread_id: str):
    try:
        agents = get_all_agents()
        for agent in agents:
            await agent.graph.checkpointer.adelete_thread(thread_id)
        return {"status": "success", "message": f"Thread {thread_id} å·²æ¸…é™¤"}
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤å¤±è´¥: {str(e)}")

#å¤„ç†ç—…äººä¿¡æ¯ç»“æ„åŒ–çš„æ¥å£
@app.post("/extract_patient_info", response_model=PatientInfoResponse)
async def process_patient_info(request: PatientInfoRequest):
    try:
        # è°ƒç”¨ç»“æ„åŒ–å¤„ç†æ¨¡å‹
        result = PatientInfoDescriptionAgent.invoke({"patient_info":request.patient_info})
        # è¿”å›ç»“æ„åŒ–åçš„ä¿¡æ¯
        return PatientInfoResponse(structured_info=result.model_dump())
    except Exception as e:
        logger.error(f"å¤„ç†ç—…äººä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))
